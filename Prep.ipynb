{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fff9e87-747b-4dec-9218-af519359f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When calling Large Language Models (LLMs), the temperature parameter is a hyperparameter that controls the randomness or uncertainty of the model's output. It's typically denoted by the symbol \"τ\" (tau) or \"T\".\n",
      "\n",
      "**What does the temperature parameter do?**\n",
      "\n",
      "The temperature parameter influences the probability distribution over the possible output tokens. When the temperature is:\n",
      "\n",
      "* **High** (e.g., τ = 1.0 or higher): The model produces more random and diverse output. The probability distribution is more uniform, and the model is more likely to generate novel or unexpected responses.\n",
      "* **Low** (e.g., τ = 0.1 or lower): The model produces more deterministic and predictable output. The probability distribution is more peaked, and the model is more likely to generate responses that are similar to the training data.\n",
      "\n",
      "In other words, the temperature parameter controls the trade-off between:\n",
      "\n",
      "1. **Exploration** (high temperature): The model is more likely to explore new possibilities and generate novel responses.\n",
      "2. **Exploitation** (low temperature): The model is more likely to exploit the knowledge it has learned from the training data and generate more predictable responses.\n",
      "\n",
      "**Common temperature values:**\n",
      "\n",
      "* τ = 0.7 to 1.0: A good starting point for many applications, as it balances exploration and exploitation.\n",
      "* τ = 0.1 to 0.5: Useful for applications where you want more predictable output, such as text summarization or question answering.\n",
      "* τ = 1.0 to 2.0: Useful for applications where you want more creative or diverse output, such as text generation or dialogue systems.\n",
      "\n",
      "Keep in mind that the optimal temperature value depends on the specific use case, model architecture, and dataset. You may need to experiment with different temperature values to find the best one for your application.\n",
      "\n",
      "I hope this helps! Do you have any other questions about the temperature parameter or LLMs in general?\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "with open(\"apikey.txt\", \"r\") as file:\n",
    "    apiKey = file.read().replace(\"\\n\", \"\")\n",
    "    \n",
    "llm = ChatGroq(temperature=0.7, groq_api_key=apiKey, model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "response = llm.invoke(\"can you tell me what temperature param do while calling llms\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c156958-48d4-4f56-9a30-9c3563dffe48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
